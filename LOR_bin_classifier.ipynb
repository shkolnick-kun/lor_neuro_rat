{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This file is a part of the lor_neuro_rat project.\n",
    "    Copyright (C) 2019 anonimous\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the GNU General Public License\n",
    "    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\n",
    "    Please contact with me by E-mail: shkolnick.kun@gmail.com\n",
    "    \n",
    "    This file is based on \n",
    "    <https://www.kaggle.com/larryfreeman/toxic-comments-code-for-alexander-s-9872-model>\n",
    "    which has been released under the Apache 2.0 open source license by Larry Freeman <https://www.kaggle.com/larryfreeman>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "\n",
    "import json\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(32)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, recall_score, f1_score, accuracy_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Flatten\n",
    "from keras.layers import Add, Concatenate\n",
    "from keras.layers import Dropout, SpatialDropout1D, BatchNormalization\n",
    "from keras.layers import GRU, LSTM, Bidirectional, SeparableConv1D, Dense\n",
    "from keras.layers import MaxPooling1D, AveragePooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "\n",
    "from keras.preprocessing.text import text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from keras import optimizers\n",
    "from keras.layers import Lambda\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import os\n",
    "#os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "#from unidecode import unidecode\n",
    "\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from tensorflow import ConfigProto\n",
    "from tensorflow import Session\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сформируем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/XyWrdTok1с.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())\n",
    "print(df.describe())\n",
    "df['TokCnt'].hist(bins=100)\n",
    "df['WrdCnt'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка имбеддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Use'].values\n",
    "X = [' '.join(tokens) for tokens in list(df['Tokens'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0])\n",
    "print(X[1000])\n",
    "print(y[0])\n",
    "print(y[15000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Приделываем токенайзер из keras\n",
    "max_features = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X)\n",
    "print(len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrds = []\n",
    "idxs = []\n",
    "for w, i in tokenizer.word_index.items():\n",
    "    wrds.append(w)\n",
    "    idxs.append(i)\n",
    "    \n",
    "if 'рашка' in wrds:\n",
    "    print(idxs[wrds.index('рашка')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Сохраняем токенайзер\n",
    "with open('models/tokenizer_bin_learn.json', 'w+', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer.to_json(), ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Грузим токенайзер\n",
    "with open('models/tokenizer_bin_learn.json', 'r') as f:\n",
    "    tokenizer = text.tokenizer_from_json(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 150\n",
    "X_seq = tokenizer.texts_to_sequences(X)\n",
    "X_seq_pad = pad_sequences(X_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('data/Xy.h5', 'w')\n",
    "print(h5f.create_dataset('X_seq_pad', data=X_seq_pad))\n",
    "print(h5f.create_dataset('y', data=y))\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('data/Xy.h5', 'r')\n",
    "X_seq_pad = h5f['X_seq_pad'][:]\n",
    "y         = h5f['y'][:]\n",
    "h5f.close()\n",
    "print(X_seq_pad.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Вычисляем матрицу весов слоя имбеддингов\n",
    "from pymagnitude import *\n",
    "import gensim\n",
    "\n",
    "mg = Magnitude('Vectors/araneum_none_fasttextskipgram_300_5_2018/araneum_none_fasttextskipgram_300_5_2018.magnitude')\n",
    "ft = gensim.models.fasttext.FastText.load('Vectors/araneum_none_fasttextskipgram_300_5_2018/araneum_none_fasttextskipgram_300_5_2018.model')\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index)+1)\n",
    "embedding_matrix = np.zeros((nb_words,300))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    #\n",
    "    if i >= max_features: \n",
    "        continue\n",
    "    #\n",
    "    #embedding_matrix[i] = mg.query(word)\n",
    "    try:\n",
    "        embedding_matrix[i] = ft.wv[word]\n",
    "    except:\n",
    "        embedding_matrix[i] = mg.query(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Сохраняем матрицу имбеддингов\n",
    "h5f = h5py.File('models/EmbeddingMtx.h5', 'w')\n",
    "print(h5f.create_dataset('EmbeddingMtx', data=embedding_matrix))\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Грузим матрицу имбеддингов\n",
    "h5f = h5py.File('models/EmbeddingMtx.h5', 'r')\n",
    "embedding_matrix = h5f['EmbeddingMtx'][:]\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mg.most_similar(embedding_matrix[41]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Взято отсюда: https://www.kaggle.com/larryfreeman/toxic-comments-code-for-alexander-s-9872-model\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1, cv_iter=0, arch=0):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val, self.Weigths = validation_data\n",
    "        self.max_score = 0\n",
    "        self.not_better_count = 0\n",
    "        self._cv_iter = cv_iter\n",
    "        self._arch = arch\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=1, batch_size=512)\n",
    "            score = roc_auc_score(self.y_val, y_pred, sample_weight=self.Weigths)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "            if (score > self.max_score):\n",
    "                print(\"*** New High Score (previous: %.6f) \\n\" % self.max_score)\n",
    "                model.save(\"models/best_model_%d_%d.h5\"%(int(self._arch),int(self._cv_iter)))\n",
    "                self.max_score=score\n",
    "                self.not_better_count = 0\n",
    "            else:\n",
    "                self.not_better_count += 1\n",
    "                if self.not_better_count > 5:\n",
    "                    print(\"Epoch %05d: early stopping, high score = %.6f\" % (epoch,self.max_score))\n",
    "                    self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Взято отсюда: https://www.kaggle.com/larryfreeman/toxic-comments-code-for-alexander-s-9872-model\n",
    "#Model 0\n",
    "nn_arch = 0\n",
    "def get_model(clipvalue=1.,num_filters=64,dropout=0.1,embed_size=300):\n",
    "    inp = Input(shape=(None, ))\n",
    "    \n",
    "    x = Embedding(nb_words, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = SpatialDropout1D(dropout)(x)\n",
    "    \n",
    "    x, x_h, x_c = Bidirectional(GRU(num_filters, return_sequences=True, return_state = True, activation='elu'))(x)  \n",
    "    \n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    x = Concatenate()([avg_pool, x_h, x_c, max_pool])\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    outp = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    adam = optimizers.adam(clipvalue=clipvalue,lr=0.001)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  weighted_metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Взято отсюда: https://www.kaggle.com/larryfreeman/toxic-comments-code-for-alexander-s-9872-model\n",
    "#Model 1\n",
    "nn_arch = 1\n",
    "def get_model(clipvalue=1.,num_filters=64,dropout=0.1,embed_size=300):\n",
    "    inp = Input(shape=(None, ))\n",
    "    \n",
    "    x = Embedding(nb_words, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = SpatialDropout1D(dropout)(x)\n",
    "    \n",
    "    # Тут мы получаем конкатенацию LSTM по двум направлениям \n",
    "    # плюс Hidden state и Cell state по двум направлениям\n",
    "    x, x_a, x_b, x_c, x_d = Bidirectional(LSTM(num_filters, return_sequences=True, return_state = True))(x)  \n",
    "    \n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    x = Concatenate()([avg_pool, x_a, x_b, x_c, x_d, max_pool])\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    outp = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    adam = optimizers.adam(clipvalue=clipvalue)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  weighted_metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Взято отсюда: https://www.kaggle.com/larryfreeman/toxic-comments-code-for-alexander-s-9872-model\n",
    "#Model 2\n",
    "nn_arch = 2\n",
    "def get_model(clipvalue=1.,num_filters=64, dropout=0.2,embed_size=300):\n",
    "    inp = Input(shape=(None, ))\n",
    "    \n",
    "    x = Embedding(nb_words, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = SpatialDropout1D(dropout)(x)\n",
    "    x = Bidirectional(LSTM(num_filters, return_sequences=True))(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = SpatialDropout1D(dropout)(x)\n",
    "    x, x_h, x_c = Bidirectional(GRU(num_filters, return_sequences=True, return_state = True, activation='elu'))(x)\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    x = Concatenate()([avg_pool, x_h, x_c, max_pool])\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    outp = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    adam = optimizers.adam(clipvalue=clipvalue)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  weighted_metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Взято отсюда: https://www.kaggle.com/larryfreeman/toxic-comments-code-for-alexander-s-9872-model\n",
    "#Model 0\n",
    "nn_arch = 3\n",
    "def get_model(clipvalue=1.,num_filters=128,dropout=0.1,embed_size=300):\n",
    "    inp = Input(shape=(None, ))\n",
    "    \n",
    "    x = Embedding(nb_words, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = SpatialDropout1D(dropout)(x)\n",
    "    \n",
    "    x_2 = SeparableConv1D(embed_size, 2, activation='elu', padding='same')(x)\n",
    "    x_3 = SeparableConv1D(embed_size, 3, activation='elu', padding='same')(x)\n",
    "    x = Add()([x, x_2, x_3])\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = SpatialDropout1D(dropout)(x)\n",
    "    x, x_h, x_c = Bidirectional(GRU(num_filters, return_sequences=True, return_state = True, activation='elu'))(x)\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    x = Concatenate()([avg_pool, max_pool, x_h, x_c])\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    outp = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    adam = optimizers.adam(clipvalue=clipvalue,lr=0.001)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  weighted_metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Взято отсюда: https://www.kaggle.com/larryfreeman/toxic-comments-code-for-alexander-s-9872-model\n",
    "#Model 0\n",
    "nn_arch = 4\n",
    "def get_model(clipvalue=1.,num_filters=128,dropout=0.2,embed_size=300):\n",
    "    inp = Input(shape=(None, ))\n",
    "    \n",
    "    x = Embedding(nb_words, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = SpatialDropout1D(dropout)(x)\n",
    "    x_2 = SeparableConv1D(embed_size, 2, activation='elu', padding='same')(x)\n",
    "    #x_3 = SeparableConv1D(embed_size, 3, activation='elu', padding='same')(x)\n",
    "    x = Concatenate()([x, x_2])\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    x = Concatenate()([avg_pool, max_pool])\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    outp = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    adam = optimizers.adam(clipvalue=clipvalue,lr=0.001)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  weighted_metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Взято отсюда: https://www.kaggle.com/larryfreeman/toxic-comments-code-for-alexander-s-9872-model\n",
    "#Model 0\n",
    "nn_arch = 5\n",
    "def get_model(clipvalue=1.,num_filters=128,dropout=0.5,embed_size=300):\n",
    "    inp = Input(shape=(None, ))\n",
    "    \n",
    "    x = Embedding(nb_words, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = SpatialDropout1D(dropout)(x)\n",
    "    x_2 = SeparableConv1D(embed_size, 2, activation='elu', padding='same')(x)\n",
    "    \n",
    "    x_b = GRU(embed_size, return_sequences=True, activation='elu')(x)\n",
    "    x = Add()([x, x_b])\n",
    "    \n",
    "    x_2 = BatchNormalization()(x_2)\n",
    "    x_2 = SpatialDropout1D(dropout)(x_2)\n",
    "    x_2b = GRU(embed_size, return_sequences=True, activation='elu')(x_2)\n",
    "    x_2 = Add()([x_2, x_2b])\n",
    "    #x_3 = SeparableConv1D(embed_size, 3, activation='elu', padding='same')(x)\n",
    "    x = Concatenate()([x, x_2])\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    x = Concatenate()([avg_pool, max_pool])\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    outp = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    adam = optimizers.adam(clipvalue=clipvalue,lr=0.001)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  weighted_metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Model:' , nn_arch)\n",
    "#model = get_model()\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "# Used epochs=100 with early exiting for best score.\n",
    "epochs = 100\n",
    "gc.collect()\n",
    "K.clear_session()\n",
    "\n",
    "# Change to 4\n",
    "num_folds = 5 #number of folds\n",
    "\n",
    "#Приводим типы\n",
    "y = np.array(y)\n",
    "\n",
    "#Веса примеров, \"меняем местами\" 1 и 0\n",
    "w_0 = 1/np.sum((y == 0.0).astype('float'))\n",
    "w_1 = 1.0/np.sum((y == 1.0).astype('float'))\n",
    "w = w_0*(y == 0.0).astype('float') + w_1*(y == 1.0).astype('float')\n",
    "w /= np.sum(w)\n",
    "w *= len(y)\n",
    "\n",
    "kf = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    \n",
    "    y_train = y[train_index] \n",
    "    y_test  = y[test_index]\n",
    "    \n",
    "    X_train = X_seq_pad[train_index]\n",
    "    X_test  = X_seq_pad[test_index]\n",
    "    \n",
    "    w_train = w[train_index]\n",
    "    w_test  = w[test_index]\n",
    "    \n",
    "    model = get_model()\n",
    "    \n",
    "    ra_val = RocAucEvaluation(validation_data=(X_test, y_test, w_test), interval = 1, cv_iter=i, arch=nn_arch)\n",
    "    \n",
    "    model.fit(X_train, y_train, sample_weight=w_train, batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "             callbacks = [ra_val])\n",
    "    \n",
    "    y_t = load_model(\"models/best_model_%d_%d.h5\"%(int(nn_arch),int(i))).predict(X_test, verbose=1, batch_size=512)\n",
    "    y_a = (y_t>0.5).astype('float').reshape(y_test.shape)\n",
    "    \n",
    "    print('Weighted scores')\n",
    "    print(' *    Acc:', accuracy_score(y_test, y_a, sample_weight=w_test))\n",
    "    print(' *   Prec:', precision_score(y_test, y_a, sample_weight=w_test))\n",
    "    print(' * Recall:', recall_score(y_test, y_a, sample_weight=w_test))\n",
    "    print(' *     F1:', f1_score(y_test, y_a, sample_weight=w_test))\n",
    "    print(' * RocAuc:', roc_auc_score(y_test, y_t, sample_weight=w_test))\n",
    "\n",
    "    print('Scores')\n",
    "    print(' *    Acc:', accuracy_score(y_test, y_a))\n",
    "    print(' *   Prec:', precision_score(y_test, y_a))\n",
    "    print(' * Recall:', recall_score(y_test, y_a))\n",
    "    print(' *     F1:', f1_score(y_test, y_a))\n",
    "    print(' * RocAuc:', roc_auc_score(y_test, y_t))\n",
    "\n",
    "    print('================')\n",
    "    print('FP:', np.sum((y_a != y_test).astype('float')*y_a))\n",
    "    print('FN:', np.sum((y_a != y_test).astype('float')*y_test))\n",
    "    print('TP:', np.sum((y_a == y_test).astype('float')*y_a))\n",
    "    print('TN:', np.sum((y_a == y_test).astype('float')*(1-y_test)))\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Типичные метрики на кросс-валидации\n",
    "По разым значениям соотношения TP/TN в датасете\n",
    "\n",
    "Самая удачная модель - 0\n",
    "\n",
    "## 1/1\n",
    "\n",
    "Weighted scores\n",
    " *    Acc: 0.8083041251011054\n",
    " *   Prec: 0.82168409466153\n",
    " * Recall: 0.7941489361702128\n",
    " *     F1: 0.8076819042466866\n",
    " * RocAuc: 0.8898640985074974\n",
    "\n",
    "Scores\n",
    " *    Acc: 0.8083041251011054\n",
    " *   Prec: 0.82168409466153\n",
    " * Recall: 0.7941489361702128\n",
    " *     F1: 0.8076819042466866\n",
    " * RocAuc: 0.8898640985074974\n",
    "\n",
    "## 1/2\n",
    "\n",
    "Weighted scores\n",
    " *    Acc: 0.8088235294117647\n",
    " *   Prec: 0.8343594253884491\n",
    " * Recall: 0.7696051919956733\n",
    " *     F1: 0.8006752004501336\n",
    " * RocAuc: 0.8905964539186791\n",
    "\n",
    "Scores\n",
    " *    Acc: 0.8218587093294984\n",
    " *   Prec: 0.71579476861167\n",
    " * Recall: 0.7696051919956733\n",
    " *     F1: 0.7417253062288245\n",
    " * RocAuc: 0.8905964539186791\n",
    "\n",
    "## 1/5\n",
    "\n",
    "Weighted scores\n",
    " *    Acc: 0.8105686695278971\n",
    " *   Prec: 0.8398097668483119\n",
    " * Recall: 0.7710330138445154\n",
    " *     F1: 0.8039531397478944\n",
    " * RocAuc: 0.891488846674163\n",
    " \n",
    "Scores\n",
    " *    Acc: 0.8372573687994249\n",
    " *   Prec: 0.511841640155532\n",
    " * Recall: 0.7710330138445154\n",
    " *     F1: 0.615253877204164\n",
    " * RocAuc: 0.8914888466741502\n",
    "\n",
    "## Все данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_all = load_model('models/best_model_0_0.h5').predict(X_seq_pad, verbose=1, batch_size=512)\n",
    "#y_all += load_model('models/best_model_0_1.h5').predict(X_seq_pad, verbose=1, batch_size=512)\n",
    "#y_all += load_model('models/best_model_0_2.h5').predict(X_seq_pad, verbose=1, batch_size=512)\n",
    "#y_all += load_model('models/best_model_0_3.h5').predict(X_seq_pad, verbose=1, batch_size=512)\n",
    "#y_all += load_model('models/best_model_0_4.h5').predict(X_seq_pad, verbose=1, batch_size=512)\n",
    "#y_all /= 5.0#2.0*np.average(y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#y_t = (y_all>1.0*np.average(y_all)).astype('float').reshape(y.shape)\n",
    "y_t = (y_all>0.8).astype('float').reshape(y.shape)\n",
    "\n",
    "#print('==== Weghtted scores ====')\n",
    "#print('   Acc:', accuracy_score(y, y_t, sample_weight=w))\n",
    "#print('  Prec:', precision_score(y, y_t, sample_weight=w))\n",
    "#print('Recall:', recall_score(y, y_t, sample_weight=w))\n",
    "#print('    F1:', f1_score(y, y_t, sample_weight=w))\n",
    "#print('RocAuc:', roc_auc_score(y, y_t, sample_weight=w))\n",
    "\n",
    "print('==== Scores ====')\n",
    "print('   Acc:', accuracy_score(y, y_t))\n",
    "print('  Prec:', precision_score(y, y_t))\n",
    "print('Recall:', recall_score(y, y_t))\n",
    "print('    F1:', f1_score(y, y_t))\n",
    "print('RocAuc:', roc_auc_score(y, y_all))\n",
    "\n",
    "print('================')\n",
    "print('FP:', np.sum((y_t != y).astype('float')*y_t))\n",
    "print('FN:', np.sum((y_t != y).astype('float')*y))\n",
    "print('TP:', np.sum((y_t == y).astype('float')*y_t))\n",
    "print('TN:', np.sum((y_t == y).astype('float')*(1-y)))\n",
    "print('N: ', len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FN'] = y*(y_t != y).astype('float')\n",
    "df['FP'] = y_t*(y_t != y).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fp = df[df['FP'] == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_fp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fp.to_pickle('data/Xy_fp.pkl')\n",
    "df_fp.to_excel('data/Xy_fp.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fn = df[df['FN'] == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fn.to_pickle('data/Xy_fn.pkl')\n",
    "df_fn.to_excel('data/Xy_fn.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
