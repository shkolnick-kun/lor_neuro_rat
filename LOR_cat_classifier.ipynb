{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This file is a part of the lor_neuro_rat project.\n",
    "    Copyright (C) 2019 anonimous\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the GNU General Public License\n",
    "    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\n",
    "    Please contact with me by E-mail: shkolnick.kun@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(32)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, recall_score, f1_score, accuracy_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Flatten\n",
    "from keras.layers import Add, Concatenate\n",
    "from keras.layers import Dropout, SpatialDropout1D, BatchNormalization\n",
    "from keras.layers import GRU, LSTM, Bidirectional, SeparableConv1D, Dense\n",
    "from keras.layers import MaxPooling1D, AveragePooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from keras import optimizers\n",
    "from keras.layers import Lambda\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import os\n",
    "#os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "#from unidecode import unidecode\n",
    "\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from tensorflow import ConfigProto\n",
    "from tensorflow import Session\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сформируем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/XyWrdTokCat.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())\n",
    "print(df.describe())\n",
    "df['TokCnt'].hist(bins=100)\n",
    "df['WrdCnt'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка имбеддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby('Class').size())\n",
    "\n",
    "df = df[df['Class'] != 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "#X\n",
    "X = [' '.join(tokens) for tokens in list(df['Tokens'])]\n",
    "\n",
    "#Считаем веса\n",
    "y = df['Class'].values\n",
    "\n",
    "l = LabelEncoder().fit(y)\n",
    "y = l.transform(y)\n",
    "\n",
    "cw = compute_class_weight('balanced', np.unique(y), y)\n",
    "w = np.zeros(len(y))\n",
    "\n",
    "print(cw)\n",
    "for i in range(len(y)):\n",
    "    w[i] = cw[y[i]]\n",
    "\n",
    "#Получаем y\n",
    "y = to_categorical(y)\n",
    "\n",
    "#Формируем список категорий\n",
    "class_lut = pd.read_csv('data/markup/class_lut.csv', sep = ',')\n",
    "print(class_lut.head())\n",
    "\n",
    "desc_lut = dict(zip(list(class_lut['Class'].values), list(class_lut['Desc'].values)))\n",
    "\n",
    "lreason = []\n",
    "for i in list(l.inverse_transform(list(range(len(y[0]))))):\n",
    "    lreason.append(desc_lut[i].split(' ')[0])\n",
    "\n",
    "print(lreason)\n",
    "    \n",
    "with open('models/cat_list_learn.pkl', 'wb+') as f:\n",
    "    pk.dump(lreason, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0])\n",
    "print(X[1000])\n",
    "print(X[10000])\n",
    "print(y[0])\n",
    "print(y[1000])\n",
    "print(y[10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Приделываем токенайзер из keras\n",
    "max_features = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X)\n",
    "print(len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrds = []\n",
    "idxs = []\n",
    "for w, i in tokenizer.word_index.items():\n",
    "    wrds.append(w)\n",
    "    idxs.append(i)\n",
    "    \n",
    "if 'рашка' in wrds:\n",
    "    print(idxs[wrds.index('рашка')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Сохраняем токенайзер\n",
    "with open('models/tokenizer_cat_learn.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Грузим токенайзер\n",
    "with open('models/tokenizer_cat_learn.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 150\n",
    "X_seq = tokenizer.texts_to_sequences(X)\n",
    "X_seq_pad = pad_sequences(X_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('data/XyCat.h5', 'w')\n",
    "print(h5f.create_dataset('X_seq_pad', data=X_seq_pad))\n",
    "print(h5f.create_dataset('y', data=y))\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('data/XyCat.h5', 'r')\n",
    "X_seq_pad = h5f['X_seq_pad'][:]\n",
    "y         = h5f['y'][:]\n",
    "h5f.close()\n",
    "print(X_seq_pad.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Вычисляем матрицу весов слоя имбеддингов\n",
    "from pymagnitude import *\n",
    "import gensim\n",
    "\n",
    "mg = Magnitude('Vectors/araneum_none_fasttextskipgram_300_5_2018/araneum_none_fasttextskipgram_300_5_2018.magnitude')\n",
    "ft = gensim.models.fasttext.FastText.load('Vectors/araneum_none_fasttextskipgram_300_5_2018/araneum_none_fasttextskipgram_300_5_2018.model')\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index)+1)\n",
    "embedding_matrix = np.zeros((nb_words,300))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    #\n",
    "    if i >= max_features: \n",
    "        continue\n",
    "    #\n",
    "    #embedding_matrix[i] = mg.query(word)\n",
    "    try:\n",
    "        embedding_matrix[i] = ft.wv[word]\n",
    "    except:\n",
    "        embedding_matrix[i] = mg.query(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Сохраняем матрицу имбеддингов\n",
    "h5f = h5py.File('models/EmbeddingMtxCat.h5', 'w')\n",
    "print(h5f.create_dataset('EmbeddingMtx', data=embedding_matrix))\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Грузим матрицу имбеддингов\n",
    "h5f = h5py.File('models/EmbeddingMtxCat.h5', 'r')\n",
    "embedding_matrix = h5f['EmbeddingMtx'][:]\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mg.most_similar(embedding_matrix[41]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Взято отсюда: https://www.kaggle.com/larryfreeman/toxic-comments-code-for-alexander-s-9872-model\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1, cv_iter=0, arch=0):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val, self.Weigths = validation_data\n",
    "        self.max_score = 0\n",
    "        self.not_better_count = 0\n",
    "        self._cv_iter = cv_iter\n",
    "        self._arch = arch\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=1, batch_size=512)\n",
    "            score = roc_auc_score(self.y_val, y_pred, sample_weight=self.Weigths)\n",
    "            \n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "            if (score > self.max_score):\n",
    "                print(\"*** New High Score (previous: %.6f) \\n\" % self.max_score)\n",
    "                model.save(\"models/best_model_cat_%d_%d.h5\"%(int(self._arch),int(self._cv_iter)))\n",
    "                self.max_score=score\n",
    "                self.not_better_count = 0\n",
    "            else:\n",
    "                self.not_better_count += 1\n",
    "                if self.not_better_count > 5:\n",
    "                    print(\"Epoch %05d: early stopping, high score = %.6f\" % (epoch,self.max_score))\n",
    "                    self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Взято отсюда: https://www.kaggle.com/larryfreeman/toxic-comments-code-for-alexander-s-9872-model\n",
    "#Model 0\n",
    "nn_arch = 0\n",
    "def get_model(clipvalue=1.,num_filters=16,dropout=0.1,embed_size=300):\n",
    "    inp = Input(shape=(None, ))\n",
    "    \n",
    "    x = Embedding(nb_words, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    \n",
    "    x = SpatialDropout1D(dropout)(x)\n",
    "    \n",
    "    x, x_h, x_c = Bidirectional(GRU(num_filters, return_sequences=True, return_state = True))(x)  \n",
    "    \n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    x = concatenate([avg_pool, x_h, x_c, max_pool])\n",
    "    \n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    outp = Dense(len(y[0]), activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    adam = optimizers.adam(clipvalue=clipvalue)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  weighted_metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Взято отсюда: https://www.kaggle.com/larryfreeman/toxic-comments-code-for-alexander-s-9872-model\n",
    "#Model 1\n",
    "nn_arch = 1\n",
    "def get_model(clipvalue=1.,num_filters=16,dropout=0.1,embed_size=300):\n",
    "    inp = Input(shape=(None, ))\n",
    "    \n",
    "    x = Embedding(nb_words, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    \n",
    "    x = SpatialDropout1D(dropout)(x)\n",
    "    \n",
    "    # Тут мы получаем конкатенацию LSTM по двум направлениям \n",
    "    # плюс Hidden state и Cell state по двум направлениям\n",
    "    x, x_a, x_b, x_c, x_d = Bidirectional(LSTM(num_filters, return_sequences=True, return_state = True))(x)  \n",
    "    \n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    x = concatenate([avg_pool, x_a, x_b, x_c, x_d, max_pool])\n",
    "    \n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    outp = Dense(len(y[0]), activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    adam = optimizers.adam(clipvalue=clipvalue)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  weighted_metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Взято отсюда: https://www.kaggle.com/larryfreeman/toxic-comments-code-for-alexander-s-9872-model\n",
    "#Model 2\n",
    "nn_arch = 3\n",
    "def get_model(clipvalue=1.,num_filters=40,dropout=0.5,embed_size=300):\n",
    "    inp = Input(shape=(max_len, ))\n",
    "    \n",
    "    # Layer 1: concatenated fasttext and glove twitter embeddings.\n",
    "    x = Embedding(nb_words, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    \n",
    "    # Uncomment for best result\n",
    "    # Layer 2: SpatialDropout1D(0.5)\n",
    "    x = SpatialDropout1D(dropout)(x)\n",
    "    \n",
    "    # Uncomment for best result\n",
    "    # Layer 3: Bidirectional CuDNNLSTM\n",
    "    x = Bidirectional(LSTM(num_filters, return_sequences=True))(x)\n",
    "\n",
    "\n",
    "    # Layer 4: Bidirectional CuDNNGRU\n",
    "    x, x_h, x_c = Bidirectional(GRU(num_filters, return_sequences=True, return_state = True))(x)  \n",
    "    \n",
    "    # Layer 5: A concatenation of the last state, maximum pool, average pool and \n",
    "    # two features: \"Unique words rate\" and \"Rate of all-caps words\"\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    x = concatenate([avg_pool, x_h, x_c, max_pool])\n",
    "    \n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    # Layer 6: output dense layer.\n",
    "    outp = Dense(7, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    adam = optimizers.adam(clipvalue=clipvalue)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  weighted_metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 3\n",
    "nn_arch = 5\n",
    "def get_model(clipvalue=1.,num_filters=128,dropout=0.2,embed_size=300):\n",
    "    inp = Input(shape=(max_len, ))\n",
    "    \n",
    "    x = Embedding(nb_words, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = SpatialDropout1D(dropout)(x)\n",
    "    x_2 = SeparableConv1D(embed_size, 2, activation='elu', padding='same')(x)\n",
    "    #x_3 = SeparableConv1D(embed_size, 3, activation='elu', padding='same')(x)\n",
    "    x = Concatenate()([x, x_2])\n",
    "    \n",
    "    avg_pool = AveragePooling1D(pool_size=2)(x)\n",
    "    max_pool = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Concatenate()([avg_pool, max_pool])\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = SpatialDropout1D(dropout)(x)\n",
    "    x = SeparableConv1D(num_filters, 3, activation='elu', padding='same')(x)\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    x = Concatenate()([avg_pool, max_pool])\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(len(y[0]), activation=\"elu\")(x)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    outp = Dense(len(y[0]), activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    adam = optimizers.adam(clipvalue=clipvalue)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  weighted_metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Model:' , nn_arch)\n",
    "#model = get_model()\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "# Used epochs=100 with early exiting for best score.\n",
    "epochs = 100\n",
    "gc.collect()\n",
    "K.clear_session()\n",
    "\n",
    "# Change to 4\n",
    "num_folds = 5 #number of folds\n",
    "\n",
    "#Приводим типы\n",
    "y = np.array(y)\n",
    "\n",
    "#Веса примеров уже вычислили\n",
    "\n",
    "kf = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    \n",
    "    y_train = y[train_index] \n",
    "    y_test  = y[test_index]\n",
    "    print(y_test.shape)\n",
    "    \n",
    "    X_train = X_seq_pad[train_index]\n",
    "    X_test  = X_seq_pad[test_index]\n",
    "    \n",
    "    w_train = w[train_index]\n",
    "    w_test  = w[test_index]\n",
    "    \n",
    "    model = get_model()\n",
    "    #model.summary()\n",
    "    \n",
    "    ra_val = RocAucEvaluation(validation_data=(X_test, y_test, w_test), interval = 1, cv_iter=i, arch=nn_arch)\n",
    "    \n",
    "    model.fit(X_train, y_train, sample_weight=w_train, batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "             callbacks = [ra_val])\n",
    "    \n",
    "    y_t = load_model(\"models/best_model_cat_%d_%d.h5\"%(int(nn_arch),int(i))).predict(X_test, verbose=1, batch_size=512)\n",
    "    y_a = (y_t>0.5).astype('float').reshape(y_test.shape)\n",
    "    \n",
    "    print('Weighted scores')\n",
    "    print(classification_report(np.argmax(y_test, axis=1), np.argmax(y_t, axis=1), sample_weight=w_test))\n",
    "\n",
    "    print('Scores')\n",
    "    print(classification_report(np.argmax(y_test, axis=1), np.argmax(y_t, axis=1)))\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
